{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f081d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': <hyperopt.pyll.base.Apply at 0x7f7cde8da280>,\n",
       " 'gamma': <hyperopt.pyll.base.Apply at 0x7f7cddf26250>,\n",
       " 'reg_alpha': <hyperopt.pyll.base.Apply at 0x7f7cddf26310>,\n",
       " 'reg_lambda': <hyperopt.pyll.base.Apply at 0x7f7cddf26130>,\n",
       " 'colsample_bytree': <hyperopt.pyll.base.Apply at 0x7f7cde9d5ee0>,\n",
       " 'min_child_weight': <hyperopt.pyll.base.Apply at 0x7f7cde9d5f70>,\n",
       " 'n_estimators': 180,\n",
       " 'seed': 0,\n",
       " 'tree_method': 'gpu_hist'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5102516",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4042344625.py, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_8526/4042344625.py\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    colsample_bytree=int(space['colsample_bytree'], ), tree_method': 'gpu_hist')\u001b[0m\n\u001b[0m                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from dataset_slide import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn\n",
    "import logging\n",
    "\n",
    "\n",
    "def objective(space):\n",
    "    clf=xgb.XGBClassifier(\n",
    "                    n_estimators =space['n_estimators'], max_depth = int(space['max_depth']), gamma = space['gamma'],\n",
    "                    reg_alpha = int(space['reg_alpha']),min_child_weight=int(space['min_child_weight']),\n",
    "                    colsample_bytree=int(space['colsample_bytree'], ), tree_method': 'gpu_hist')\n",
    "\n",
    "    evaluation = [( X, y)]\n",
    "    \n",
    "    clf.fit(X, y,\n",
    "            eval_set=evaluation, eval_metric=\"mlogloss\",\n",
    "            early_stopping_rounds=10,verbose=False)\n",
    "\n",
    "\n",
    "    pred = clf.predict(X)\n",
    "    f1_score = (pred == y).mean() #sklearn.metrics.f1_score(pred, y, average='macro') #(pred == y).mean()#\n",
    "\n",
    "    print(\"f1: {}\".format(sklearn.metrics.f1_score(pred, y, average='macro')))\n",
    "\n",
    "    return {'loss': -f1_score, 'status': STATUS_OK }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffd2a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                       | 0/10 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/don/anaconda3/envs/cuda11/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.23342679751652312                                            \n",
      " 10%| | 1/10 [00:14<02:06, 14.02s/trial, best loss: -0.875657400450"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/don/anaconda3/envs/cuda11/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.23342679751652312                                            \n",
      " 20%|▏| 2/10 [00:29<01:58, 14.79s/trial, best loss: -0.875657400450"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/don/anaconda3/envs/cuda11/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.23342679751652312                                            \n",
      " 30%|▎| 3/10 [00:44<01:45, 15.07s/trial, best loss: -0.875657400450"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/don/anaconda3/envs/cuda11/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.23342679751652312                                            \n",
      " 40%|▍| 4/10 [00:57<01:25, 14.18s/trial, best loss: -0.875657400450"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/don/anaconda3/envs/cuda11/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.23342679751652312                                            \n",
      " 50%|▌| 5/10 [01:11<01:10, 14.09s/trial, best loss: -0.875657400450"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/don/anaconda3/envs/cuda11/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.23342679751652312                                            \n",
      " 60%|▌| 6/10 [01:25<00:56, 14.03s/trial, best loss: -0.875657400450"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/don/anaconda3/envs/cuda11/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.23342679751652312                                            \n",
      " 70%|▋| 7/10 [01:39<00:42, 14.09s/trial, best loss: -0.875657400450"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/don/anaconda3/envs/cuda11/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.23342679751652312                                            \n",
      " 80%|▊| 8/10 [01:53<00:27, 13.94s/trial, best loss: -0.875657400450"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/don/anaconda3/envs/cuda11/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.23342679751652312                                            \n",
      " 90%|▉| 9/10 [02:06<00:13, 13.87s/trial, best loss: -0.875657400450"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/don/anaconda3/envs/cuda11/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.23342679751652312                                            \n",
      "100%|█| 10/10 [02:20<00:00, 14.06s/trial, best loss: -0.87565740045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/don/anaconda3/envs/cuda11/lib/python3.8/site-packages/xgboost/core.py:430: FutureWarning: Pass `objective` as keyword args.  Passing these as positional arguments will be considered as error in future releases.\n",
      "  warnings.warn(\n",
      "/home/don/anaconda3/envs/cuda11/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00:10] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8271260666806587\n",
      "> \u001b[0;32m/tmp/ipykernel_8526/4271453457.py\u001b[0m(75)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     73 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     74 \u001b[0;31m\u001b[0;32mfor\u001b[0m \u001b[0minit_seed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 75 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mcv_seed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     76 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     77 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> f1\n",
      "0.8271260666806587\n",
      "ipdb> X.shape\n",
      "(2662, 114)\n",
      "ipdb> y.Sshape\n",
      "*** AttributeError: 'numpy.ndarray' object has no attribute 'Sshape'\n",
      "ipdb> y.shape\n",
      "(2662,)\n",
      "ipdb> x_test.shape\n",
      "(665, 114)\n",
      "ipdb> y_true\n",
      "array([0, 0, 0, 0, 3, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 3,\n",
      "       0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 2, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n",
      "       0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 3, 0, 0, 0,\n",
      "       0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0,\n",
      "       1, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0,\n",
      "       0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 2, 0, 1, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "       0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
      "       0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0,\n",
      "       0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n",
      "       0, 0, 0, 0, 0])\n",
      "ipdb> (y_pred == y_true).mean()     \n",
      "0.9368421052631579\n",
      "ipdb> sklearn.metrics.f1_score(y_pred, y_true, average='macro')\n",
      "0.8271260666806587\n",
      "ipdb> clf.predict(x_test)\n",
      "array([0, 0, 0, 0, 3, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 3,\n",
      "       0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 2, 0, 0, 0, 2, 0,\n",
      "       0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 2,\n",
      "       0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 0,\n",
      "       0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0,\n",
      "       1, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 2, 0, 0, 3, 3, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 2, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "       0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3,\n",
      "       0, 0, 0, 0, 0, 0, 3, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0,\n",
      "       0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n",
      "       0, 0, 0, 0, 0])\n",
      "ipdb> clf.predict(x_test) == y_pred\n",
      "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True])\n"
     ]
    }
   ],
   "source": [
    "person_order = {'F1_Interaction_1': {'P2': 1, 'P1': 1, 'P3': 2},\n",
    " 'F1_Interaction_2': {'P2': 1, 'P1': 1, 'P3': 2},\n",
    " 'F2_Interaction_1': {'P4': 1, 'P5': 3},\n",
    " 'F2_Interaction_2': {'P4': 1},\n",
    " 'F3_Interaction_1': {'P8': 3, 'P6': 1, 'P7': 1},\n",
    " 'F3_Interaction_2': {'P6': 1, 'P7': 1},\n",
    " 'F4_Interaction_1': {'P14': 2,\n",
    "  'P12': 1,\n",
    "  'P11': 1,\n",
    "  'P10': 1,\n",
    "  'P9': 1,\n",
    "  'P13': 3},\n",
    " 'F4_Interaction_2': {'P12': 1,\n",
    "  'P11': 1,\n",
    "  'P10': 1,\n",
    "  'P9': 1,\n",
    "  'P13': 3},\n",
    " 'F5_Interaction_1': {'P16': 2, 'P15': 1},\n",
    " 'F5_Interaction_2': {'P16': 2, 'P15': 1},\n",
    " 'F6_Interaction_1': {'P19': 3, 'P18': 1, 'P17': 1},\n",
    " 'F6_Interaction_2': {'P19': 3, 'P18': 1, 'P17': 1},\n",
    " 'F7_Interaction_1': {'P22': 3,\n",
    "  'P20': 1,\n",
    "  'P21': 1,\n",
    "  'P23': 2},\n",
    " 'F8_Interaction_1': {'P24': 1, 'P25': 3},\n",
    " 'F8_Interaction_2': {'P24': 1, 'P25': 3},\n",
    " 'F8_Interaction_3': {'P24': 1, 'P25': 3},\n",
    " 'F10_Interaction_1': {'P27': 1, 'P28': 1},\n",
    " 'F11_Interaction_1': {'P29': 1, 'P30': 2},\n",
    " 'F11_Interaction_2': {'P29': 1, 'P30': 2},\n",
    " 'F13_Interaction_1': {'P32': 1, 'P33': 2},\n",
    " 'F17_Interaction_1': {'P37': 1, 'P38': 2},\n",
    " 'F17_Interaction_2': {'P37': 1, 'P38': 2}}\n",
    "\n",
    "\n",
    "group_nums = {1: ['F2_Interaction_2'],\n",
    " 2: ['F2_Interaction_1',\n",
    "  'F3_Interaction_2',\n",
    "  'F5_Interaction_1',\n",
    "  'F5_Interaction_2',\n",
    "  'F8_Interaction_1',\n",
    "  'F8_Interaction_2',\n",
    "  'F8_Interaction_3',\n",
    "  'F10_Interaction_1',\n",
    "  'F11_Interaction_1',\n",
    "  'F11_Interaction_2',\n",
    "  'F13_Interaction_1',\n",
    "  'F17_Interaction_1',\n",
    "  'F17_Interaction_2'],\n",
    " 3: ['F1_Interaction_1',\n",
    "  'F1_Interaction_2',\n",
    "  'F3_Interaction_1',\n",
    "  'F6_Interaction_1',\n",
    "  'F6_Interaction_2'],\n",
    " 4: ['F7_Interaction_1'],\n",
    " 5: ['F4_Interaction_2'],\n",
    " 6: ['F4_Interaction_1']}\n",
    "\n",
    "group_all_dataset = []\n",
    "group_ids = group_nums[3]\n",
    "for group_id in group_ids:\n",
    "    group_specific_dataset = SpeedDatingDS(group_id = group_id, social_rel = person_order[group_id])\n",
    "    group_all_dataset.append(group_specific_dataset)\n",
    "\n",
    "SD = torch.utils.data.ConcatDataset(group_all_dataset)\n",
    "\n",
    "########################################################################\n",
    "#Dataloader\n",
    "########################################################################\n",
    "train_len = len(SD) - len(SD)//5\n",
    "test_len = len(SD)//5\n",
    "\n",
    "for init_seed in [0,1,2]:\n",
    "    for cv_seed in [0,1,2,3,4]:\n",
    "\n",
    "\n",
    "        np.random.seed(init_seed)\n",
    "        index_list = np.arange(len(SD))\n",
    "        np.random.shuffle(index_list)\n",
    "        test_range = index_list[list(range(test_len*(cv_seed), test_len*(cv_seed+1)))]\n",
    "        train_range = index_list[list(set(range(len(SD))) - set(test_range))]\n",
    "        train = torch.utils.data.Subset(SD, train_range)\n",
    "        test = torch.utils.data.Subset(SD, test_range)\n",
    "\n",
    "        batch_size = 32\n",
    "        trainloader = DataLoader(train, batch_size = train_len, shuffle = True, num_workers = 2)\n",
    "        testloader = DataLoader(test, batch_size = test_len, shuffle = True, num_workers = 2)\n",
    "\n",
    "\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        for idx, batch in enumerate(trainloader):\n",
    "\n",
    "            x_train, vb_output = batch['context'], batch['vb_output']\n",
    "\n",
    "            labels = vb_output.sum(2).to(device).flatten(start_dim =1)\n",
    "            index_labels = torch.zeros(x_train.shape[0]).long().to(device)\n",
    "            index_labels[labels.nonzero()[:,0]] = labels.nonzero()[:,1] + 1 \n",
    "            y_train = index_labels\n",
    "\n",
    "        for idx, batch in enumerate(testloader):\n",
    "            x_test, vb_output = batch['context'], batch['vb_output']\n",
    "\n",
    "            labels = vb_output.sum(2).to(device).flatten(start_dim =1)\n",
    "            index_labels = torch.zeros(x_test.shape[0]).long().to(device)\n",
    "            index_labels[labels.nonzero()[:,0]] = labels.nonzero()[:,1] + 1 \n",
    "            y_test = index_labels\n",
    "\n",
    "        X = x_train.flatten(start_dim =1).cpu().numpy()\n",
    "        y = y_train.cpu().numpy()\n",
    "\n",
    "        x_test = x_test.flatten(start_dim =1).cpu().numpy()\n",
    "        y_test = y_test.cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "        space={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n",
    "                'gamma': hp.uniform ('gamma', 1,9),\n",
    "                'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n",
    "                'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "                'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n",
    "                'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "                'n_estimators': 180,\n",
    "                'seed': 0,\n",
    "                'tree_method': 'gpu_hist'\n",
    "            }\n",
    "\n",
    "        trials = Trials()\n",
    "\n",
    "        tested_best_hyperparams = fmin(fn = objective,\n",
    "                                space = space,\n",
    "                                algo = tpe.suggest,\n",
    "                                max_evals = 10,\n",
    "                                trials = trials)\n",
    "\n",
    "        clf = xgb.XGBClassifier(tested_best_hyperparams)\n",
    "\n",
    "        clf.fit(X, y)\n",
    "        y_true, y_pred = y_test, clf.predict(x_test)\n",
    "\n",
    "        f1 = sklearn.metrics.f1_score(y_pred, y_true, average='macro')\n",
    "        acc = (y_pred == y_true).mean()     \n",
    "        print(f1)\n",
    "    #     print(\"f1: {}\".format()))\n",
    "    #     print(\"weighted_f1: {}\".format(sklearn.metrics.f1_score(y_pred, y_true, average='weighted')))\n",
    "    #     print(\"acc: {}\".format((y_pred == y_true).mean()))\n",
    "    #     print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "        model_unique = \"{model}\".format(model = \"XGBoost_init_seed{}\".format(init_seed) + \"_cv_seed{}\".format(cv_seed))\n",
    "\n",
    "        clf.save_model(\"./rebuttal_model_weights/{}.json\".format(model_unique))\n",
    "        \n",
    "        log_dir = \"./rebuttal_logs_fixed_hyperparams/\"\n",
    "        log_path = log_dir + \"{model}.log\".format(model = model_unique)\n",
    "        with open(log_path, 'w') as f:\n",
    "            f.write(\"NextSpeaker\")\n",
    "            f.write('\\n')\n",
    "            f.write(\"{}\".format(model_unique))\n",
    "            f.write('\\n')\n",
    "            f.write(\"acc: {acc}, f1:{f1} \".format(acc = acc, f1 = f1))\n",
    "            pdb.set_trace()\n",
    "        \n",
    "#         logging.basicConfig(filename=log_path)\n",
    "#         log_statement = \"NextSpeaker\"\n",
    "#         logging.warning(log_statement)\n",
    "#         log_statement = \"{}\".format(model_unique)\n",
    "#         logging.warning(log_statement)\n",
    "#         print(model_unique)\n",
    "#         print(\"acc: {acc}, f1:{f1} \".format(acc = acc, f1 = f1))\n",
    "#         log_statement = \"acc: {acc}, f1:{f1} \".format(acc = acc, f1 = f1)\n",
    "#         logging.warning(log_statement)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80567e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/don/anaconda3/envs/cuda11/lib/python3.8/site-packages/xgboost/core.py:430: FutureWarning: Pass `objective` as keyword args.  Passing these as positional arguments will be considered as error in future releases.\n",
      "  warnings.warn(\n",
      "/home/don/anaconda3/envs/cuda11/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:33:51] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(tested_best_hyperparams)\n",
    "\n",
    "clf.fit(X, y)\n",
    "y_true, y_pred = y_test, clf.predict(x_test)\n",
    "\n",
    "f1 = sklearn.metrics.f1_score(y_pred, y_true, average='macro')\n",
    "acc = (y_pred == y_true).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "152a07b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8271260666806587"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3da0d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tested_best_hyperparams = {'colsample_bytree': 0.8965848062170019,\n",
    " 'gamma': 3.6363645213941274,\n",
    " 'max_depth': 8.0,\n",
    " 'min_child_weight': 3.0,\n",
    " 'reg_alpha': 141.0,\n",
    " 'reg_lambda': 0.7080398610996163}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8adc69e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/don/anaconda3/envs/cuda11/lib/python3.8/site-packages/xgboost/core.py:430: FutureWarning: Pass `objective` as keyword args.  Passing these as positional arguments will be considered as error in future releases.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8526/3321539242.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtested_best_hyperparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(tested_best_hyperparams)\n",
    "clf.fit(X, y)\n",
    "y_true, y_pred = y_test, clf.predict(x_test)\n",
    "\n",
    "f1 = sklearn.metrics.f1_score(y_pred, y_true, average='macro')\n",
    "acc = (y_pred == y_true).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
